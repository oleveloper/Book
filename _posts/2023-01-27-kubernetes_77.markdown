---
layout: post
title: "Kubernetes_77 Configure High Availability"
date: 2023-01-27
last_modified_at: 2023-01-27
categories: [Kubernetes]
tags: [Kubernetes]
---

# Configure High Availability

Kubernetes를 고가용성 모드로 구성하는 것에 대해 이야기해보자.   
HA 모드에서 etcd에 대해 이야기 할 것이며, 특히 클러스터의 etcd 구성에 초점을 맞출 것이다.   
이제 클러스터의 노드 수, raft 프로토콜이 무엇인지 등을 간단히 요약해보도록 하자.   

etcd가 무엇인가? 단순하고, 안전하며, 빠른 분산형 reliable key value store이다.   
전통적으로 데이터는 테이블에 정리되어 저장된다. 예를 들어, 여러 명의 사람에 대한 세부적인 정보를 저장한다.   
key-value 저장소는 문서 또는 페이지 형태로 정보를 저장한다.   
그래서 각각은 documents 또는 pages를 얻고, 각 사람에 대한 모든 정보는 그 파일에 저장된다.   
이러한 파일은 형식이나 구조에 상관없이 사용할 수 있으며 한 파일을 변경해도 다른 파일에는 영향을 주지 않는다.   

아래의 경우 근로자는 급여 필드가 있는 파일을 가질 수 있다.   
<img width="1181" alt="스크린샷 2023-01-27 오후 11 13 17" src="https://user-images.githubusercontent.com/83587720/215107536-859682f1-1430-4d4c-a32e-7d09ba792798.png">   
간단한 키와 값을 저장하고 검색할 수 있지만, 데이터가 복잡해지면 일반적으로 JSON 또는 YAML과 같은 데이터 형식으로 트랜잭션을 수행하게 된다.   
이것이 etcd가 무엇인지에 대한 설명이다.   

etcd는 배포된다고 말한 적이 있다. 이게 무슨 뜻일까?   
앞선 포스팅에서는 단일 서버에 etcd를 가지고 있었지만, 그것은 데이터베이스이고 아마도 중요한 데이터를 저장할 것이다. 따라서 여러 서버에 걸쳐 데이터 저장소를 보유할 수 있다.   
<img width="712" alt="스크린샷 2023-01-27 오후 11 16 05" src="https://user-images.githubusercontent.com/83587720/215108105-d0abaa1e-73aa-4379-a4b2-c42767be4c92.png">   
   
이제 3개의 서버가 모두 etcd를 실행하고 데이터베이스의 동일한 복사본을 유지한다.   
하나를 분실해도 데이터 복사본이 두 개 남아 있다.   

그렇지만 모든 노드의 데이터가 일관되도록 하려면 어떻게 해야 할까?    
etcd는 모든 인스턴스에서 동일한 일관된 데이터 복사본을 동시에 사용할 수 있도록 보장한다.   
어떻게 그럴 수 있을까? Read만 한다면 쉽게 가능할 것이다.   
모든 노드에서 동일한 데이터를 사용할 수 있으므로 모든 노드에서 쉽게 읽을 수 있다.   
하지만 Write의 경우는 그렇지 않다.   
두 개의 쓰기 요청이 두 개의 서로 다른 인스턴스에서 발생하면 어떻게 될까? 어떤 것이 통과할까?    
예를 들어, 한 쪽 에는 'John', 다른 쪽에는 'Joe'로 이름에 대한 글이 들어왔다고 가정해보자.   
물론 두 개의 서로 다른 노드에 두 개의 서로 다른 데이터가 있을 수는 없다.   
모든 인스턴스에서 etcd에 Write 할 수 있다고 생각했다면, 그것은 틀린 생각이다.   
etcd는 각각의 노드에서 Write를 수행하지 않는다.   
인스턴스 중 하나만 Write를 수행한다.   
내부적으로 두 노드는 그 중에서 리더를 선출한다.   
전체 인스턴스 중에서 한 노드가 리더가 되고 다른 노드가 팔로워가 된다.   
<img width="816" alt="스크린샷 2023-01-27 오후 11 21 53" src="https://user-images.githubusercontent.com/83587720/215109246-6e618cd0-906b-4123-af5e-66d515029971.png">   

Write가 leader 노드를 통해 들어온 경우 leader는 Write 프로세스를 수행한다.   
리더는 다른 노드가 데이터 복사본을 전송받는지 확인한다.   
Write가 팔로워 노드를 통해 들어온 경우 내부적으로 쓰기를 리더에게 전달한 다음 리더가 Write를 처리한다.   
다시말해, Write가 처리될 때 리더는 쓰기 복사본이 클러스터의 다른 인스턴스로 배포되도록 한다.   
따라서 Write는 leader가 클러스터의 다른 구성원으로부터 동의를 얻은 경우에만 완료된 것으로 간주된다.   

## RAFT Algorithm
그렇다면 그들은 어떻게 그들 사이에서 지도자를 선출할까? 그리고 쓰기가 모든 인스턴스에 전파되도록 하려면 어떻게 해야 할까? etcd는 RAFT 프로토콜을 사용하여 분산 합의를 구현한다.   

<img width="543" alt="스크린샷 2023-01-27 오후 11 28 11" src="https://user-images.githubusercontent.com/83587720/215110622-931383fb-f0bc-44b1-ad2e-e32831d16d7a.png">

### Leader 선출
- Term
논리적인 시계. 자신이 얼마나 권위 있는 서버인지를 보여주는 정수 값.
<img width="313" alt="스크린샷 2023-01-27 오후 11 28 26" src="https://user-images.githubusercontent.com/83587720/215110670-d4784c20-bdf3-4cdf-bb06-fb52e5819ccd.png">

- Election Timeout
Follower가 Candidate로 변환되는 데 까지 걸리는 시간.
- Heartbeat
Leader가 모든 Follower에게 반복적으로 전달하는 메시지.

#### Process
1. Cluster에 Leader가 없을 때에는 모든 node가 Follower 상태이다.
모든 node는 Election Timeout까지 대기한다.
2. Election Timeout이 가장 먼저 끝난 노드가 Candidate로 전환된다.
Term이 시작되고, Candidate는 자신의 노드에 한 표를 준 후 다른 노드들에 투표 요청한다.
3. 2에서 투표 요청 메시지를 받은 노드가 해당 Term 중에 아직 투표한 적이 없다면 Candidate에게 투표 메시지를 보낸 후 자신의 Election Timeout을 초기화한다. 
4. 전체 노드 수의 과반수에 해당하는 응답을 얻은 노드는 해당 Term에 대한 Leader로 선출된다. 
어떠한 노드도 과반을 얻지 못했다면 Term을 종료하고 새 Term이 시작되며 선거가 다시 시작된다.

### 로그 복제
#### Process
1. Client가 Request하면 이를 Leader가 수신한다. 
2. Leader는 Request를 Log에 추가하고, Follower들도 각자의 Log에 추가하도록 한다.
3. Leader가 명령이 실행되었다고 생각하면, 리더는 FSM을 사용하여 명령을 실행한다.
4. 실행한 결과를 Client에 회신한다. 
5. Leader는 Offset의 가장 높은 값을 추적하여 Follower에 Offset과 함께 요청을 보낸다.
6. Follower는 이 Offset값까지의 모든 명령을 FSM을 사용해 실행한다.

- Quorum
(N + 1) / 2 와 같거나 큰 자연수에 해당하는 수를 의미한다.
    - `N`이 홀수(`2k+1`)일 때 : **`k`개 노드**의 장애까지 허용 가능
    - `N`이 짝수(`2k`)일 때 : **`k-1`개 노드**의 장애까지 허용 가능
